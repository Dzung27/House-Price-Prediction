{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12087708,"sourceType":"datasetVersion","datasetId":7609287}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/datast/cleaned_dataset.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.astype('int64')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nnumerical_cols = ['Amount', 'Price', 'Area']\n\nscaler = StandardScaler()\ndf[numerical_cols] = scaler.fit_transform(df[numerical_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = np.array(df.drop(['Price'], axis=1))\ny = np.array(df['Price'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x.shape)\nprint(y.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\nfrom scipy.stats import multivariate_normal\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass EntropicOptimalTransportNPMLE:\n    \"\"\"\n    Non-Parametric Maximum Likelihood Estimation using Entropic Optimal Transport\n    Based on Weed & Rigollet (2018) \"A notion of duality between entropy and the central limit theorem\"\n    \"\"\"\n    \n    def __init__(self, reg_param=0.05, max_iter=200, tol=1e-5, \n                 max_support_size=25, bandwidth='scott'):\n        \"\"\"\n        Parameters:\n        -----------\n        reg_param : float\n            Entropic regularization parameter (epsilon in Sinkhorn)\n        max_iter : int\n            Maximum iterations for Sinkhorn algorithm\n        tol : float\n            Convergence tolerance\n        max_support_size : int\n            Maximum number of support points to consider\n        bandwidth : str or float\n            Bandwidth for Gaussian kernels ('scott', 'silverman', or float)\n        \"\"\"\n        self.reg_param = reg_param\n        self.max_iter = max_iter\n        self.tol = tol\n        self.max_support_size = max_support_size\n        self.bandwidth = bandwidth\n        \n        # Fitted parameters\n        self.support_points_ = None\n        self.weights_ = None\n        self.n_components_ = 0\n        self.cost_matrix_ = None\n        self.transport_plan_ = None\n        \n    def _compute_bandwidth(self, X):\n        \"\"\"Compute bandwidth for Gaussian kernels\"\"\"\n        n_samples, n_features = X.shape\n        \n        if self.bandwidth == 'scott':\n            # Scott's rule\n            return n_samples ** (-1.0 / (n_features + 4))\n        elif self.bandwidth == 'silverman':\n            # Silverman's rule\n            return (n_samples * (n_features + 2) / 4.0) ** (-1.0 / (n_features + 4))\n        else:\n            return float(self.bandwidth)\n    \n    def _select_support_points(self, X):\n        \"\"\"\n        Select candidate support points using K-means clustering\n        This reduces computational complexity while maintaining coverage\n        \"\"\"\n        n_samples = X.shape[0]\n        \n        if n_samples <= self.max_support_size:\n            return X.copy()\n        \n        # Use K-means to select representative points\n        kmeans = KMeans(n_clusters=self.max_support_size, random_state=42, n_init=10)\n        kmeans.fit(X)\n        \n        return kmeans.cluster_centers_\n    \n    def _compute_cost_matrix(self, X, Y, bandwidth):\n        \"\"\"\n        Compute cost matrix for optimal transport\n        Uses squared Euclidean distance scaled by bandwidth\n        \"\"\"\n        # Squared Euclidean distances\n        distances_sq = cdist(X, Y, metric='sqeuclidean')\n        \n        # Scale by bandwidth\n        cost_matrix = distances_sq / (2 * bandwidth**2)\n        \n        return cost_matrix\n    \n    def _sinkhorn_algorithm(self, cost_matrix, a, b, reg_param, max_iter, tol):\n        \"\"\"\n        Sinkhorn algorithm for entropic optimal transport\n        \n        Parameters:\n        -----------\n        cost_matrix : array, shape (n, m)\n            Cost matrix C[i,j] = cost of transporting from i to j\n        a : array, shape (n,)\n            Source distribution (marginal constraint)\n        b : array, shape (m,)\n            Target distribution (marginal constraint)\n        reg_param : float\n            Entropic regularization parameter\n        \"\"\"\n        n, m = cost_matrix.shape\n        \n        # Initialize dual variables\n        u = np.ones(n) / n\n        v = np.ones(m) / m\n        \n        # Kernel matrix K = exp(-C/reg_param)\n        K = np.exp(-cost_matrix / reg_param)\n        \n        # Avoid numerical issues\n        K = np.maximum(K, 1e-100)\n        \n        for iteration in range(max_iter):\n            u_prev = u.copy()\n            \n            # Update u\n            u = a / (K @ v)\n            u = np.maximum(u, 1e-100)  # Avoid division by zero\n            \n            # Update v\n            v = b / (K.T @ u)\n            v = np.maximum(v, 1e-100)  # Avoid division by zero\n            \n            # Check convergence\n            if np.linalg.norm(u - u_prev) < tol:\n                break\n        \n        # Compute transport plan\n        transport_plan = np.diag(u) @ K @ np.diag(v)\n        \n        return transport_plan, u, v\n    \n    def _gaussian_kernel_density(self, X, support_points, bandwidth):\n        \"\"\"\n        Compute Gaussian kernel density at support points\n        \"\"\"\n        n_samples, n_features = X.shape\n        n_support = len(support_points)\n        \n        densities = np.zeros(n_support)\n        \n        for i, support_point in enumerate(support_points):\n            # Compute Gaussian kernel density\n            diff = X - support_point\n            distances_sq = np.sum(diff**2, axis=1)\n            \n            # Gaussian kernel\n            kernel_values = np.exp(-distances_sq / (2 * bandwidth**2))\n            normalizing_constant = (2 * np.pi * bandwidth**2) ** (n_features / 2)\n            \n            densities[i] = np.mean(kernel_values) / normalizing_constant\n        \n        return densities\n    \n    def _extract_mixture_components(self, transport_plan, support_points, threshold=1e-6):\n        \"\"\"\n        Extract Gaussian mixture components from transport plan\n        \"\"\"\n        n_samples, n_support = transport_plan.shape\n        \n        # Sum transport plan over samples to get weights at support points\n        support_weights = np.sum(transport_plan, axis=0)\n        \n        # Keep only support points with significant weight\n        significant_indices = support_weights > threshold\n        active_support_points = support_points[significant_indices]\n        active_weights = support_weights[significant_indices]\n        \n        # Normalize weights\n        if len(active_weights) > 0:\n            active_weights = active_weights / np.sum(active_weights)\n        \n        return active_support_points, active_weights\n    \n    def fit(self, X, y=None):\n        \"\"\"\n        Fit NPMLE using Entropic Optimal Transport\n        \"\"\"\n        n_samples, n_features = X.shape\n        \n        # Standardize data\n        self.scaler_ = StandardScaler()\n        X_scaled = self.scaler_.fit_transform(X)\n        \n        print(f\"Fitting EOT-NPMLE on {n_samples} samples with {n_features} features\")\n        \n        # Step 1: Select support points\n        print(\"Selecting support points...\")\n        candidate_support_points = self._select_support_points(X_scaled)\n        n_support = len(candidate_support_points)\n        print(f\"Selected {n_support} candidate support points\")\n        \n        # Step 2: Compute bandwidth\n        bandwidth = self._compute_bandwidth(X_scaled)\n        print(f\"Computed bandwidth: {bandwidth:.6f}\")\n        \n        # Step 3: Compute cost matrix\n        print(\"Computing cost matrix...\")\n        self.cost_matrix_ = self._compute_cost_matrix(\n            X_scaled, candidate_support_points, bandwidth\n        )\n        \n        # Step 4: Set up marginal constraints\n        # Source: uniform distribution over data points\n        a = np.ones(n_samples) / n_samples\n        \n        # Target: Gaussian kernel density at support points\n        b = self._gaussian_kernel_density(X_scaled, candidate_support_points, bandwidth)\n        b = b / np.sum(b)  # Normalize\n        \n        # Step 5: Solve entropic optimal transport\n        print(\"Solving entropic optimal transport...\")\n        self.transport_plan_, u, v = self._sinkhorn_algorithm(\n            self.cost_matrix_, a, b, self.reg_param, self.max_iter, self.tol\n        )\n        \n        # Step 6: Extract mixture components\n        print(\"Extracting mixture components...\")\n        active_support_points, active_weights = self._extract_mixture_components(\n            self.transport_plan_, candidate_support_points\n        )\n        \n        # Step 7: Estimate covariance for each component\n        components = []\n        final_weights = []\n        \n        for i, (support_point, weight) in enumerate(zip(active_support_points, active_weights)):\n            if weight > 1e-6:  # Only keep significant components\n                # Estimate local covariance using weighted samples\n                distances = np.sum((X_scaled - support_point)**2, axis=1)\n                \n                # Use transport plan to weight nearby points\n                transport_weights = self.transport_plan_[:, i] if i < self.transport_plan_.shape[1] else None\n                \n                if transport_weights is not None and np.sum(transport_weights) > 1e-6:\n                    # Weighted covariance\n                    transport_weights = transport_weights / np.sum(transport_weights)\n                    centered = X_scaled - support_point\n                    cov = np.average(centered[:, :, np.newaxis] * centered[:, np.newaxis, :], \n                                   weights=transport_weights, axis=0)\n                else:\n                    # Local covariance using k-nearest neighbors\n                    k_nearest = min(50, n_samples)\n                    nearest_indices = np.argsort(distances)[:k_nearest]\n                    nearest_points = X_scaled[nearest_indices]\n                    cov = np.cov(nearest_points.T)\n                \n                # Add regularization\n                cov += 1e-6 * np.eye(n_features)\n                \n                components.append({\n                    'mean': support_point,\n                    'cov': cov\n                })\n                final_weights.append(weight)\n        \n        # Store results\n        self.support_points_ = components\n        self.weights_ = final_weights\n        self.n_components_ = len(components)\n        \n        print(f\"Found {self.n_components_} components\")\n        print(f\"Component weights: {[f'{w:.4f}' for w in self.weights_]}\")\n        \n        return self\n    \n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities\"\"\"\n        if self.support_points_ is None:\n            raise ValueError(\"Model not fitted yet\")\n        \n        X_scaled = self.scaler_.transform(X)\n        n_samples = X.shape[0]\n        probabilities = np.zeros((n_samples, self.n_components_))\n        \n        for i in range(n_samples):\n            total_prob = 0\n            probs = []\n            \n            for k in range(self.n_components_):\n                try:\n                    prob = self.weights_[k] * multivariate_normal.pdf(\n                        X_scaled[i],\n                        self.support_points_[k]['mean'],\n                        self.support_points_[k]['cov']\n                    )\n                    probs.append(prob)\n                    total_prob += prob\n                except:\n                    probs.append(1e-10)\n                    total_prob += 1e-10\n            \n            for k in range(self.n_components_):\n                probabilities[i, k] = probs[k] / max(total_prob, 1e-10)\n        \n        return probabilities\n    \n    def predict(self, X):\n        \"\"\"Predict cluster labels\"\"\"\n        probabilities = self.predict_proba(X)\n        return np.argmax(probabilities, axis=1)\n    \n    def score(self, X):\n        \"\"\"Compute log-likelihood\"\"\"\n        X_scaled = self.scaler_.transform(X)\n        n_samples = X.shape[0]\n        log_likelihood = 0\n        \n        for i in range(n_samples):\n            mixture_density = 0\n            for k in range(self.n_components_):\n                try:\n                    density = self.weights_[k] * multivariate_normal.pdf(\n                        X_scaled[i],\n                        self.support_points_[k]['mean'],\n                        self.support_points_[k]['cov']\n                    )\n                    mixture_density += density\n                except:\n                    mixture_density += 1e-10\n            \n            log_likelihood += np.log(max(mixture_density, 1e-10))\n        \n        return log_likelihood\n\ndef fit_eot(X, y=None, reg_param=0.05):\n    \"\"\"\n    Function to fit EOT-NPMLE on your actual data (159630, 40)\n    \n    Parameters:\n    -----------\n    X : array-like, shape (159630, 40)\n        Your feature data\n    y : array-like, shape (159630,), optional\n        Your target data\n    reg_param : float\n        Entropic regularization parameter (smaller = less regularization)\n    \"\"\"\n    \n    print(f\"Fitting EOT-NPMLE on data with shape: {X.shape}\")\n    \n    # For large datasets, use PCA for dimensionality reduction\n    if X.shape[1] > 20:\n        print(\"Applying PCA for dimensionality reduction...\")\n        pca = PCA(n_components=15, random_state=42)\n        X_reduced = pca.fit_transform(X)\n        print(f\"Reduced dimensionality from {X.shape[1]} to {X_reduced.shape[1]}\")\n        print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n    else:\n        X_reduced = X\n        pca = None\n    \n    # Initialize model with parameters suitable for large datasets\n    eot_npmle = EntropicOptimalTransportNPMLE(\n        reg_param=reg_param,  # Entropic regularization\n        max_iter=500,         # Reduced for efficiency\n        tol=1e-5,            # Slightly relaxed tolerance\n        max_support_size=15, # Limit support points for computational efficiency\n        bandwidth='scott'     # Automatic bandwidth selection\n    )\n    \n    # Fit the model\n    eot_npmle.fit(X_reduced, y)\n    eot_npmle.pca_ = pca  # Store PCA for predictions\n    \n    # Get results\n    def predict_original(X_orig):\n        if pca is not None:\n            X_transformed = pca.transform(X_orig)\n            return eot_npmle.predict(X_transformed)\n        else:\n            return eot_npmle.predict(X_orig)\n    \n    def predict_proba_original(X_orig):\n        if pca is not None:\n            X_transformed = pca.transform(X_orig)\n            return eot_npmle.predict_proba(X_transformed)\n        else:\n            return eot_npmle.predict_proba(X_orig)\n    \n    cluster_labels = predict_original(X)\n    probabilities = predict_proba_original(X)\n    \n    print(f\"\\nResults:\")\n    print(f\"Number of components found: {eot_npmle.n_components_}\")\n    print(f\"Component weights: {[f'{w:.4f}' for w in eot_npmle.weights_]}\")\n    print(f\"Unique cluster labels: {np.unique(cluster_labels)}\")\n    print(f\"Cluster distribution: {np.bincount(cluster_labels)}\")\n    \n    return {\n        'model': eot_npmle,\n        'pca': pca,\n        'cluster_labels': cluster_labels,\n        'probabilities': probabilities,\n        'n_components': eot_npmle.n_components_,\n        'weights': eot_npmle.weights_,\n        'predict_fn': predict_original,\n        'predict_proba_fn': predict_proba_original\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport numpy as np\n\ndef evaluate_eot_with_gboost(X, y, n_splits=5, reg_param=0.1, random_state=42):\n    \"\"\"\n    Evaluate EOT-NPMLE for regression using Gradient Boosting and PCA on cluster probabilities.\n\n    Parameters:\n    -----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix\n    y : array-like, shape (n_samples,)\n        Target values\n    n_splits : int\n        Number of cross-validation folds\n    reg_param : float\n        Entropic regularization parameter for EOT\n    random_state : int\n        Random seed for reproducibility\n\n    Returns:\n    --------\n    results : dict\n        Evaluation metrics and models per fold\n    \"\"\"\n\n    results = {\n        'mse': [], 'r2': [], 'mae': [],\n        'models': [], 'test_preds': [], 'test_true': []\n    }\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n=== Fold {fold + 1}/{n_splits} ===\")\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # Fit EOT-NPMLE\n        print(\"Fitting EOT-NPMLE...\")\n        eot_result = fit_eot(X_train, y_train, reg_param=reg_param)\n\n        # Get cluster probabilities\n        train_probs = eot_result['probabilities']\n        test_probs = eot_result['predict_proba_fn'](X_test)\n\n        # Reduce dimensions of probability features\n        print(\"Applying PCA...\")\n        pca = PCA(n_components=8, random_state=random_state)\n        train_pca = pca.fit_transform(train_probs)\n        test_pca = pca.transform(test_probs)\n\n        # Train Gradient Boosting Regressor\n        print(\"Training Gradient Boosting model...\")\n        gboost = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=random_state)\n        gboost.fit(train_pca, y_train)\n\n        # Predict\n        y_pred = gboost.predict(test_pca)\n\n        # Evaluate\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        mae = mean_absolute_error(y_test, y_pred)\n\n        print(f\"Fold {fold + 1} Results:\")\n        print(f\"- MSE: {mse:.4f}\")\n        print(f\"- R²: {r2:.4f}\")\n        print(f\"- MAE: {mae:.4f}\")\n\n        results['mse'].append(mse)\n        results['r2'].append(r2)\n        results['mae'].append(mae)\n        results['models'].append((eot_result['model'], gboost))\n        results['test_preds'].append(y_pred)\n        results['test_true'].append(y_test)\n\n    # Summary\n    results['mean_mse'] = np.mean(results['mse'])\n    results['std_mse'] = np.std(results['mse'])\n    results['mean_r2'] = np.mean(results['r2'])\n    results['std_r2'] = np.std(results['r2'])\n    results['mean_mae'] = np.mean(results['mae'])\n    results['std_mae'] = np.std(results['mae'])\n\n    print(\"\\n=== Final Evaluation ===\")\n    print(f\"Average MSE: {results['mean_mse']:.4f} ± {results['std_mse']:.4f}\")\n    print(f\"Average R²: {results['mean_r2']:.4f} ± {results['std_r2']:.4f}\")\n    print(f\"Average MAE: {results['mean_mae']:.4f} ± {results['std_mae']:.4f}\")\n\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\ndef plot_regression_results(results):\n    \"\"\"Visualize regression results\"\"\"\n    plt.figure(figsize=(15, 5))\n    \n    # Plot predictions vs true values\n    plt.subplot(1, 3, 1)\n    all_preds = np.concatenate(results['test_preds'])\n    all_true = np.concatenate(results['test_true'])\n    plt.scatter(all_true, all_preds, alpha=0.3)\n    plt.plot([min(all_true), max(all_true)], [min(all_true), max(all_true)], 'r--')\n    plt.xlabel(\"True Values\")\n    plt.ylabel(\"Predicted Values\")\n    plt.title(\"Predicted vs True Values\")\n    \n    # Plot metric distributions\n    plt.subplot(1, 3, 2)\n    metrics = ['mse', 'r2', 'mae']\n    metric_names = ['MSE', 'R²', 'MAE']\n    for metric, name in zip(metrics, metric_names):\n        plt.plot(results[metric], 'o-', label=name)\n    plt.xlabel(\"Fold\")\n    plt.ylabel(\"Metric Value\")\n    plt.title(\"Metric Values Across Folds\")\n    plt.legend()\n    \n    # Plot error distribution\n    plt.subplot(1, 3, 3)\n    errors = all_preds - all_true\n    sns.histplot(errors, kde=True)\n    plt.xlabel(\"Prediction Error\")\n    plt.title(\"Error Distribution\")\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = evaluate_eot_with_gboost(x, y, n_splits=5, reg_param=0.1, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_regression_results(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}